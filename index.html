<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autograd Tutorial</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&family=Roboto+Slab:wght@400;700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.css" />
  
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    <div class="container">
        <header class="tutorial-header">
            <h1>A Fully Annotated Autograd Tutorial</h1>
            <p class="author-date">Author: Michael Kosmider | Published: May 7, 2025</p>
        </header>

        <nav class="toc">
            <h2>Table of Contents</h2>
            <ul>
                <li><a href="#setup">Setup and the Chain Rule</a></li>
            </ul>
        </nav>

        <main class="tutorial-content">

            <p>
                In this tutorial, you will learn the theory behind reverse mode automatic differentiation, and see how to implement a real autograd system in practice. We will start with a theoretical overview and move on to implement the modules you'd typically see in an autograd framework, from basic functions like addition and matrix multiplication, to linear and convolution layers. This tutorial assumes the reader is familiar with multivariable calculus and strong with NumPy. Familiarity with basic PyTorch modules is also recommended, as this system will have a similar interface. 
            </p>

            <p>
                Finally, you'll see how to use this system to train a ResNet50 on the CIFAR10 dataset! Let's get started.
            </p>
            <section id="setup">
                <h2>Setup and the Chain Rule</h2>

                <p>
                    From the point of view of an autograd system, there are two main entities: variables and functions. A variable can be a scalar, a matrix, or a tensor of arbitrary dimensions. A function \(f\) is a mapping 
                </p>
                <p>
                    \[
                    f(V_1, V_2, \ldots,V_k)\rightarrow V_o
                    \]
                </p>

                <p>
                    from a fixed-size list of variables to a single variable. However, for now we will assume that all variables are real valued scalars, and that all functions output scalars, in order to develop the needed theory. Then, we will expand the theory to include the general formulation of variables and functions above. 
                </p>

                <p>
                    Functions can be composed in the sense that the output variable of one function is used as an input variable of another. In this way one can construct a computation graph, where the nodes are variables, and for each variable pair \(x,y,\) there exists an edge if \(x\) is an input to a function whose output is \(y\). An example of such a graph is depicted below.

                        \[
                        \begin{align}
                            y_1 &= f_1(x_1, x_2, x_3, x_4) = x_1+ x_2 +x_3 + x_4\\
                            y_2 &= f_2(x_4) = x_4^2\\
                            z &= f_3(x_1, y_1, y_2) = x_1 \cdot y_1 + y_2\\
                        \end{align}
                        \]

                </p>

                <figure id="fig-attention-graph" style="text-align: center;">

                    <img class="image-placeholder" style="width: 50%; height: auto;" src="images/computation_graph.png" alt="computation graph example">

                    <figcaption><strong>Figure 1:</strong> Computation graph example.</figcaption>

                </figure>

                <p>
                    There are two key takeaways from this example. Notice first that there are no cycles in the graph. All compositions of functions must respect this rule, meaning that their graphs are directed acyclic graphs (DAGs). Second, as demonstrated by \(x_1\) and \(x_4\), variables are allowed to be inputs to more than just one function. 
                </p>

                <p>
                    We're interested in developing a system that, given a computation graph, can calculate the partial derivative of any variable in the graph with respect to any of its ancestors. For this, we must study the inductively defined chain rule.
                </p>

                <section id="theorem-chain-rule">

                    <div class="theorem-box">
                        <div class="theorem-title">Theorem (Inductive Chain Rule on a Computation Graph)</div>

                        <p>
                        Let \( G = (V, E) \) be a computation graph, meaning that:
                        </p>

                        <ol>
                        <li>\(V\) is the set of all defined variables.</li>
                        <li>For all pairs of variables \( x,y \in V\), if a function takes \(x\) as an input and outputs \(y\), then \((x,y) \in E\).</li>
                        </ol>

                        <p>
                        Then, let \( x, z \in V \) be two variables such that \( z \) is a descendant of \( x \). That is, there exists a directed path from \( x \) to \( z \) in the computation graph. Finally, let \(c(x)\) denote the set of children of \(x\). 
                        </p>

                        <p>
                        Then the derivative of \( z \) with respect to \( x \) is given by:
                        </p>

                        <div id="eq-chain-rule">
                            <p style="text-align: center; font-size: 1.2em;">
                            \[
                            \frac{dz}{dx} = \sum_{y \in c(x)} \frac{dz}{dy} \cdot \frac{\partial y}{\partial x}
                            \tag{1}
                            \]
                            </p>
                        </div>

                        <p>
                            where we take the derivative of a variable with respect to itself to be \(1\). 
                        </p>
                        
                    </div>

                    <p>
                        
                    </p>

                    <p>
                        Let's think about what this theorem means for us. Suppose we are attemping to calculate the derivative of some variable \(z\) with respect to an ancestor \(x\), and furthermore we have <b>already calculated</b> \(\frac{dz}{dy}\) for all \(y \in c(x)\). Then all that remains in order to obtain \(\frac{dz}{dx}\) is to first calculate \(\frac{\partial y}{\partial x}\) for all children \(y \in c(x)\), and to invoke <a href="#eq-chain-rule">Equation (1)</a>. This is the heart of backpropagation. If we know the derivative of \(z\) with respect to all the children of \(x\), then the derivative of \(z\) with respect to \(x\) is straightforward to calculate.
                    </p>

                    <p>
                        Going back to the example in <a href="#fig-attention-graph">Figure 1</a>, let's think about how we would obtain \(\frac{\partial z}{\partial x_1}\). The children of \(x\) are \(c(x) = \{y_1, z\}\), and thus the inductive chain rule gives us

                        <div id="chain-rule-example">
                        \[
                        \begin{align}
                            \frac{dz}{dx_1} &= \frac{dz}{dz} \cdot \frac{\partial z}{\partial x_1} + \frac{dz}{dy_1} \cdot \frac{\partial y_1}{\partial x_1}\tag{2} \\
                            &= 1 \cdot y_1 + x_1 \cdot 1\\
                            &= y_1 + x_1
                        \end{align}
                        \]
                        </div>    
                    </p >

                    <div style="font-size: 0.9em; color: #555; margin-top: 0.5em; padding-left: 1em; border-left: 3px solid #ccc;">
                        A word on notation, the difference between \(\frac{dz}{dx_1}\) and \(\frac{\partial z}{\partial x_1}\) is important. The former represents the total derivative, whereas the latter represents a direct functional dependence. 
                    </div>

                    <p>
                        Since this is a simple example, we were able to replace each derivative in <a href="#eq-chain-rule">Equation (2)</a> with a directly computed value, and our invokation of the inductive chain rule was unsystematic, but the example shows a glimpse of its power. In the next subsection, we'll explore the algorithm at the heart of automatic differentiation. 
                    </p>

                    <h3>Backpropagation</h3>
                    <p>
                        Suppose instead we had an elaborate computation graph with thousands of variables and functions. Suppose also that \(z\) is some variable in the computation graph and that we'd like to calculate the derivative of \(z\) with respect to every single ancestor of \(z\). We'd need a method that systematically applies the chain rule to make these calculations. This is exactly what backpropagation accomplishes. 
                    </p>

                    <p>
                        Think about the earlier statement, that said if we have already calculated \(\frac{dz}{dy}\) for all children \(y\) of \(x\), then the inductive chain rule can be invoked to calculate the \(\frac{dz}{dx}\). What we'd like then, is an ordering of all ancestors of \(z\) such that if we calculated the derivatives in that order, each ancestor would have its children's derivatives calculated before itself.
                    </p>

                    <p>
                        The algorithm that returns this ordering is called topological sort. I recommend learning about this algorithm if you are not familiar with it, there are many resources online. By triggering topological sort on \(z\), treating edges as dependencies, we obtain an ordering such that for any ancestor \(x\) of \(z\), \(x\) appears after in the ordering after its own children. Backpropagation then calculates derivatives in this order, and keeps track of all computed derivatives at each step in a dictionary. The pseudocode is shown below.
                    </p>

                
                    <pre class="line-numbers item" style="font-size: 0.70em;"><code class="language-python">BACKPROPAGATION(Computation graph G, node z in G):

    Let topo_order = TopologicalSort(G, z)

    Let computed = {dz/dz : 1}

    For each node x in topo_order (excluding z):

        Let dz/dx = 0

        # Invoke Inductive Chain Rule
        For each child y of x:
            Let partial_x = calculate(∂y/∂x)
            Let total_y = computed.get(dz/dy)
            dz/dx += total_y * partial_x

        insert dz/dx into computed</code></pre>
    

                <p>
                    Returning to the computation graph in <a href="#fig-attention-graph">Figure 1</a>, the ordering returned by topological sort might be \((z, y_2, y_1, x_2, x_1, x_3, x_4)\). Other orderings are possible too, as multiple orderings satisfy the dependencies in this case. From here, the loop on line 7 would proceed:
                </p>

                <ol>

                    <li>
                        <p>current node: \(y_2\)</p>
                        <p>computed = \(\{\frac{dz}{dz} : 1\}\) </p>
                        <p>calculate: \(\frac{\partial z}{\partial y_2} = 1\) </p>
                        <p>invoke chain rule: \(\frac{dz}{dy_2} = \frac{dz}{dz} \cdot \frac{\partial z}{\partial y_2} = 1 \cdot 1 = 1\)</p>
                    </li>

                    <li>
                        <p>current node: \(y_1\)</p>
                        <p>computed = \(\{\frac{dz}{dz} : 1, \frac{dz}{dy_2} : 1\}\) </p>
                        <p>calculate: \(\frac{\partial z}{\partial y_1} = x_1\) </p>
                        <p>invoke chain rule: \(\frac{dz}{dy_1} = \frac{dz}{dz} \cdot \frac{\partial z}{\partial y_1} = 1 \cdot x_1 = x_1\)</p>

                    </li>

                    <li>
                        <p>current node: \(x_2\)</p>
                        <p>computed = \(\{\frac{dz}{dz} : 1, \frac{dz}{dy_2} : 1, \frac{dz}{dy_1} : x_1\}\) </p>
                        <p>calculate: \(\frac{\partial y_1}{\partial x_2} = 1\) </p>
                        <p>invoke chain rule: \(\frac{dz}{dx_2} = \frac{dz}{dy_1} \cdot \frac{\partial y_1}{\partial dx_2} =  x_1 \cdot 1 = x_1\)</p>

                    </li>

                    <li>
                        <p>current node: \(x_1\)</p>
                        <p>computed = \(\{\frac{dz}{dz} : 1, \frac{dz}{dy_2} : 1, \frac{dz}{dy_1} : x_1, \frac{dz}{dx_2} : x_1\}\) </p>
                        <p>calculate: \(\frac{\partial z}{\partial x_1} = y_1\) and \(\frac{\partial y_1}{\partial x_1} = 1\) </p>
                        <p>invoke chain rule: \(\frac{dz}{dx_1} = \frac{dz}{dz} \cdot \frac{\partial z}{\partial x_1} + \frac{dz}{dy_1} \cdot \frac{\partial y_1}{\partial x_1} =  1 \cdot y_1 + x_1 \cdot 1 = y_1 + x_1\)</p>
                    </li>

                    <li>
                        <p>current node: \(x_3\)</p>
                        <p>computed = \(\{\frac{dz}{dz} : 1, \frac{dz}{dy_2} : 1, \frac{dz}{dy_1} : x_1, \frac{dz}{dx_2} : x_1, \frac{dz}{dx_1} : y_1 + x_1\}\) </p>
                        <p>calculate:\(\frac{\partial y_1}{\partial x_3} = 1\) </p>
                        <p>invoke chain rule: \(\frac{dz}{dx_3} = \frac{dz}{dy_1} \cdot \frac{\partial y_1}{\partial x_3} =  x_1 \cdot 1 = x_1\)</p>
                    </li>

                    <li>
                        <p>current node: \(x_4\)</p>
                        <p>computed = \(\{\frac{dz}{dz} : 1, \frac{dz}{dy_2} : 1, \frac{dz}{dy_1} : x_1, \frac{dz}{dx_2} : x_1, \frac{dz}{dx_1} : y_1 + x_1, \frac{dz}{dx_3} : x_1\}\) </p>
                        <p>calculate: \(\frac{\partial y_1}{\partial x_4} = 1\) and \(\frac{\partial y_2}{\partial x_4} = 2x_4\) </p>
                        <p>invoke chain rule: \(\frac{dz}{dx_4} = \frac{dz}{dy_1} \cdot \frac{\partial y_1}{\partial x_4} + \frac{dz}{dy_2} \cdot \frac{\partial y_2}{\partial x_4} =  x_1 \cdot 1 + 1 \cdot 2x_4 = x_1 + 2x_4\)</p>
                    </li>

                    <li>
                        Final output: \(\{\frac{dz}{dz} : 1, \frac{dz}{dy_2} : 1, \frac{dz}{dy_1} : x_1, \frac{dz}{dx_2} : x_1, \frac{dz}{dx_1} : y_1 + x_1, \frac{dz}{dx_3} : x_1, \frac{dz}{dx_4} : x_1 + 2x_4 \}\) 
                    </li>
                </ol>

                <p>
                    At each iteration of backpropagation, for the current node \(x\), we invoke the chain rule using the previously calculated total derivatives of \(z\) with respect to children of \(x\), combined with the partial derivatives of children of \(x\) with respect to itself. At each iteration, the dictionary of computed total derivatives grows by one. 
                </p>

                <h3>Extending Backpropagation to Tensors</h3>

                    <p>
                        We just saw the inductive chain rule and backpropagation in the context of scalar variables and functions. We will now extend it to tensor valued variables and functions. This will allow us to express variables and compositions of functions in a more compact way, and it will allow us to invoke the chain rule "in bulk". We'll start with an example. 
                    </p>

                    <p>
                        Let \(A,B \in \mathbb{R}^{2 \times 2}\) be two matrices, and suppose we let \(z = \text{sum}(A \times B)\), meaning \(z\) is a scalar resulting from summing the entries of the matrix given by \(A \times B\). It makes perfect sense to for example be intersted in the derivative of \(z\) with respect to the entry \(a_{11}\) of \(A\). In fact, \(z = \text{sum}(A \times B)\) can secretly be expressed as a computation graph with scalar valued functions and variables, like we saw before:
                    </p>

                    <p>
                        \[
                            A = \begin{bmatrix}
                            a_{11} & a_{12} \\
                            a_{21} & a_{22}
                            \end{bmatrix}

                            = \begin{bmatrix}
                            3 & 7 \\
                            2 & 5
                            \end{bmatrix}

                        \]
                        \[
                            B = \begin{bmatrix}
                            b_{11} & b_{12} \\
                            b_{21} & b_{22}
                            \end{bmatrix}

                            = \begin{bmatrix}
                            2 & 0 \\
                            0 & 4
                            \end{bmatrix}
                            
                        \]
                        \[
                            C = A \times B
                             = \begin{bmatrix}
                            c_{11} & c_{12} \\
                            c_{21} & c_{22}
                            \end{bmatrix}

                            = \begin{bmatrix}
                            3*2 + 7*0 & 3*0+7*4 \\
                            2*2+5*0 & 2*0 + 5*4
                            \end{bmatrix}
                            
                            = \begin{bmatrix}
                            6 & 28 \\
                            4 & 20
                            \end{bmatrix}
                        \]
                    
                    </p>

                    <p>
                        where the computation graph is given by:
                    </p>

                    <ol>

                        <li>
                            \(c_{11} = a_{11}*b_{11} + a_{12}*b_{21}\)
                        </li>
                        <li>
                            \(c_{12} = a_{11}*b_{12} + a_{12}*b_{22}\)             
                        </li>
                        <li>
                            \(c_{21} = a_{21}*b_{11} + a_{22}*b_{21}\)   
                        </li>
                        <li>
                            \(c_{22} = a_{21}*b_{12} + a_{22}*b_{22}\)       
                        </li>
                        <li>
                            \(z = c_{11} + c_{12} + c_{21} + c_{22}\)    
                        </li>

                    </ol>
                    <p>
                        We can invoke backpropagation on this computation graph just like before, and arrive at the derivatives (try and calculate these for yourself): 
                    </p>
                    <p>
                        \(\frac{dz}{dc_{11}} = \frac{dz}{dc_{12}} = \frac{dz}{dc_{21}} = \frac{dz}{dc_{22}} = 1, \frac{dz}{da_{11}} = \frac{dz}{da_{21}} = b_{11} + b_{12}, \frac{dz}{da_{12}} = \frac{dz}{da_{22}} = b_{21} + b_{22}, \)
                        \(\frac{dz}{db_{11}} = \frac{dz}{db_{12}} = a_{11} + a_{21}, \text{ and } \frac{dz}{db_{21}} = \frac{dz}{db_{22}} = a_{12} + a_{22}.\)
                    </p>

                    <p>
                        It gets cumbersome to keep track of this many individual derivatives, thus introducing the need for a new kind of notation. Let's define the derivative of a scalar \(z\) with respect to a whole tensor \(T\) as \(\frac{dz}{dT}\), where \(\frac{dz}{dT}\) has the same shape as \(T\) itself, and where each entry in \(\frac{dz}{dT}\) is the derivative of \(z\) with respect to the corresponding entry in \(T\). So all the above derivatives can now be expressed more compactly:
                    </p>

                    <p>
                        
                        \[
                            \begin{align}
                                \frac{dz}{dA} &= \begin{bmatrix}
                                \frac{dz}{da_{11}} & \frac{dz}{da_{12}} \\
                                \frac{dz}{da_{21}} & \frac{dz}{da_{22}}
                                \end{bmatrix}

                                = \begin{bmatrix}
                                b_{11} + b_{12} & b_{21} + b_{22} \\
                                b_{11} + b_{12} & b_{21} + b_{22}
                                \end{bmatrix},\\

                                \frac{dz}{dB} &= \begin{bmatrix}
                                \frac{dz}{db_{11}} & \frac{dz}{db_{12}} \\
                                \frac{dz}{db_{21}} & \frac{dz}{db_{22}}
                                \end{bmatrix}

                                = \begin{bmatrix}
                                a_{11} + a_{21} & a_{11} + a_{21} \\
                                a_{12} + a_{22} & a_{12} + a_{22}
                                \end{bmatrix},\\
            
                                \frac{dz}{dC} &= \begin{bmatrix}
                                \frac{dz}{dc_{11}} & \frac{dz}{dc_{12}} \\
                                \frac{dz}{dc_{21}} & \frac{dz}{dc_{22}}
                                \end{bmatrix}

                                = \begin{bmatrix}
                                1 & 1 \\
                                1 & 1
                                \end{bmatrix}.\\
                            \end{align}
                        \]
                        
                    </p>

                    <p>
                        Now you may have noticed that these derivatives closely follow a pattern. Whenever the entries of a matrix have a summation pattern, your instinct should be that it is the result of multiplying two matrices, which is in fact the case here. 
                    </p>

                    <p>
                        \[
                            \begin{align}

                                \frac{dz}{dA} &= \begin{bmatrix}
                                \frac{dz}{da_{11}} & \frac{dz}{da_{12}} \\
                                \frac{dz}{da_{21}} & \frac{dz}{da_{22}}
                                \end{bmatrix}\\

                                &= \begin{bmatrix}
                                b_{11} + b_{12} & b_{21} + b_{22} \\
                                b_{11} + b_{12} & b_{21} + b_{22}
                                \end{bmatrix}\\

                                &= \begin{bmatrix}
                                1 * b_{11} + 1 * b_{12} & 1 * b_{21} + 1 * b_{22} \\
                                1 * b_{11} + 1 * b_{12} & 1 * b_{21} + 1 * b_{22}
                                \end{bmatrix}\\

                                &= 
                                 \begin{bmatrix}
                                1 &  1 \\
                                1 &  1
                                \end{bmatrix} \times
                                \begin{bmatrix}
                                b_{11} & b_{21} \\
                                b_{12} & b_{22}
                                \end{bmatrix}\\

                                &= \frac{dz}{dC} \times B^\mathsf{T}

                            \end{align}
                        \]
                    </p>

                    <p>
                        and similarly \(\frac{dz}{dB} = A^\mathsf{T} \times \frac{dz}{dC}\). I won't prove this here, but this result is no coincidence. It is a special case of a more general result that compactly summarizes the derivative of a scalar with respect to a matrix that was multiplied with another matrix. 
                    </p>

                    <div class="theorem-box">
                        <div class="theorem-title">Result (Gradient of Scalar Function of Matrix Product)</div>


                        <p>
                            Let \( A \in \mathbb{R}^{m \times n} \), \( B \in \mathbb{R}^{n \times p} \) and \( C = A \times B \). Furthermore let \( z = f(C) \) where \(f\) is a scalar valued differentiable function. Then
                        </p>

                        <div id="matmul-backwards">
                            \[
                                \frac{dz}{dA} = \frac{dz}{dC} \times B^{\mathsf{T}}, \tag{3} \quad \text{and} \quad
                                \frac{dz}{dB} = A^{\mathsf{T}} \times \frac{dz}{dC}.
                            \]
                        </div>
                    </div>

                    <p>
                        This is a very exciting result, and hints at how autodiff systems actually make computations. The pseudocode backpropagation algorithm I showed earlier is theoretically sound, and it is the most general form of backpropagation where every edge is explicitly present in the computation graph, but it is not how backpropagation is implemented in practice. Instead, autodiff uses formulas like those shown above to apply the scalar chain rule "in bulk". The computation graph then does not need to keep track of every edge and scalar entry in every tensor. Our autodiff system will express the example in this section like so:
                    </p>
                    <p class="math-scrollable">
                        \[
                        \begin{array}{ccccccccc}
                        A& \searrow & & &&\\
                        B & \rightarrow & \text{matmul} & \rightarrow & C & \rightarrow & \text{sum} & \rightarrow & z\\
                        \end{array}
                        \]
                    </p>

                    <p>
                        Then knowing \(z = \text{sum}(C)\) it will reason that \(\frac{dz}{dC} =  \begin{bmatrix}
                                1 &  1 \\
                                1 &  1
                                \end{bmatrix}\) 

                        , and finally knowing \(C = \text{matmul}(A,B)\), it will apply <a href="#matmul-backwards">Equations (3)</a> thus obtaining \(\frac{dz}{dA}\) and \(\frac{dz}{dB}\).
                     </p>   

                     <p>
                        More generally, functions in an autodiff system take lists of tensors as input and produce a tensor as output. If autodiff knows the derivative of a scalar \(z\) with respect to a function \(f\)'s output (also known as the upstream gradient of \(f\)), then autodiff uses what's called the "backward function" of \(f\) to compute the derivative of \(z\) with respect to the \(f\)'s inputs. This backward function is typically a clever result that implicitly invokes the inductive chain rule upon all scalars involved.
                    </p>

                    <p>
                        This approach to autodiff introduces a problem. It is also possible for tensors to be used as input to more than one function. Suppose we augmented the example by letting 
                        
                            \(
                                D = 
                                \begin{bmatrix}
                                    5 &  3 \\
                                    1 &  9
                                \end{bmatrix}
                           
                        , E = B + D, W = C + E,\) and finally \(z = \text{sum}(W)\) . The computation graph would change to 
                    </p>

                    <p class="math-scrollable">
                        \[
                        \begin{array}{ccccccccc}
                        A& \searrow & & &&\\
                        B & \rightarrow & \text{matmul} & \rightarrow & C & \searrow & \\
                        & \searrow&&&&&\text{add} & \rightarrow & W & \text{sum} & \rightarrow & z\\
                        D & \rightarrow &\text{add} & \rightarrow &E &\nearrow \\ 
                        \end{array}
                        \]
                    </p>

                    <p>
                        Now we can no longer apply <a href="#matmul-backwards">Equations (3)</a> in order to obtain \(\frac{dz}{dB}\) because \(z\) can no longer be expressed as a direct function of just \(C\). 
                    </p>

                    <p>
                        Looking at it from a different angle, if we let \(\frac{dz}{dB} = A^{\mathsf{T}} \times \frac{dz}{dC}\) as before, we would no longer be correctly invoking the inductive chain rule. That is because the scalars in \(B\) have children not only in \(C\), but also in \(E\). Remember, the inductive chain rule dictates that for any scalar \(b_{ij} \in B\), 
                    </p>

                    <p>
                        \[
                        \frac{dz}{db_{ij}} = \sum_{y \in c(b_{ij})} \frac{dz}{dy} \cdot \frac{\partial y}{\partial b_{ij}}
                        \]
                    </p>

                    <p>
                        Which we can break down further by letting \(c_{C}(b_{ij})\) denote the children of \(b_{ij}\) that are in \(C\), and similarly defining \(c_{E}(b_{ij})\) 
                    </p>

                    <p>
                        \[
                        \frac{dz}{db_{ij}} = \sum_{y \in c_{C}(b_{ij})} \frac{dz}{dy} \cdot \frac{\partial y}{\partial b_{ij}} + 
                        \sum_{y \in c_{E}(b_{ij})} \frac{dz}{dy} \cdot \frac{\partial y}{\partial b_{ij}}.
                        \]
                    </p>

                    <p>
                        In other words, if we let \(\frac{dz}{dB} = A^{\mathsf{T}} \times \frac{dz}{dC}\), then each entry \(\frac{dz}{db_{ij}}\) in \(\frac{dz}{dB}\) would be of the form 
                    </p>

                    <p>
                        \[
                        \frac{dz}{db_{ij}} = \sum_{y \in c_{E}(b_{ij})} \frac{dz}{dy} \cdot \frac{\partial y}{\partial b_{ij}}.
                        \]
                    </p>

                    <p>
                        which is only the first half of the summation! But this also shows what the next step must be.
                    </p>
                </section>
                <!-- <p class="item">
                    from a fixed-size list of variables to a single variable. The variable at index \(i\) must come from a pre-defined set, and the order of the input list matters. For example, \(V_2\) might always have to be a scalar, and \(V_4\) might always be a two dimensional tensor with \(8\) columns (but any number of rows). The set of acceptable input lists of variables is the domain of \(f\), and it the domain depends on how the function is defined.
                </p>
                <p class="item">
                    Variables and functions can be composed in the sense that the output of a function can be used as input to another function, and the composition can be viewed as a computation graph, where the nodes are variables and the functions are edges. Let's look at a typical example: a linear layer followed by a ReLU activation.
                </p>
                <p class="math-scrollable">
                    \[
                    \begin{array}{ccccccccc}
                      X & \searrow &&&&&&&\\
                      W & \rightarrow & \text{Linear} & \rightarrow & Z & \rightarrow & \text{ReLU} & \rightarrow & A\\
                      b & \nearrow &&&&&&&\\
                    \end{array}
                    \]
                </p>
                <p>
                    where \(X \in \mathbb{R}^{N \times d}\),&nbsp \(W \in \mathbb{R}^{d \times c}\),&nbsp \(b \in \mathbb{R}^c\),&nbsp and \(Z, A \in \mathbb{R}^{N \times c}\), where \(b\) and \(c\) are fixed integers, whereas \(N\) is not fixed and is the "batch size".
                </p>
                <p>
                    Functions can be composed in any way so long as there is no cycle in the computation graph. The computation graph is what's called a directed acyclic graph.
                </p>
            </section id="Autograd and the Chain Rule">
                <h2>Backpropagation and the Chain Rule</h2>

                <p>
                    The goal of autograd is as follows: Given a scalar variable \(s\) somewhere in a computation graph, and for each ancestor variable \(X\), calculate the gradient \(\frac{\partial s}{\partial X}\). The gradient \(\frac{\partial s}{\partial X}\) is always the same shape as \(X\) itself, and if \(X\) is a tensor, then each entry in the gradient is the partial derivative of \(s\) with respect to that entry in \(X\).  
                </p>

                <p>
                    Let's continue the example from above to illustrate this concept. Extend the computation graph starting from \(A\), to also include a softmax function and a cross entropy function. 
                </p>
        
                <p class="math-scrollable">
                    \[
                    \begin{array}{ccccccccc}
                       &&&&y& \searrow & & &&\\
                      A & \rightarrow & \text{softmax} & \rightarrow & P & \rightarrow & \text{cross entropy} & \rightarrow & L\\
                    \end{array}
                    \]
                </p>
                <p>
                    You might know that cross entropy is a function that takes a batch of probability predictions \(P \in \mathbb{R}^{N \times c}\), as well as a batch of corresponding ground truths \(y\), and produces a scalar loss \(L\) which is the average loss over the batch. Now we might for example be interested in calculating \(\frac{\partial L}{\partial W}\), which is the gradient of \(L\) with respect to \(W\). It has the exact same shape as \(W\) itself, and the \(ij\)-th entry is \(\frac{\partial L}{\partial W_{ij}}\). 
                </p>
                <p>
                    Autograd uses backpropagation in order to calculate \(\frac{\partial L}{\partial W}\), and we will go into a lot of detail about this example because it's foundational in the study of neural networks, but for now we'll return to calculating \(\frac{\partial s}{\partial X}\) to see how backpropagation works in a more general sense (and in practice). 
                </p>
                <p>
                    Let \(I(X)\) denote the set of functions \(f\) to which \(X\) is an input and such that \(f\) is an edge in a path from \(X\) to \(s\). It might look something like this:
                </p>
                <p class="math-scrollable">
                    \[
                    \begin{array}{}
                        &\nearrow&f_{1}&\rightarrow& Y_1 &\searrow\\
                        X&\rightarrow&f_{2}&\rightarrow& Y_2 &\rightarrow & f_{4} & \rightarrow s\\
                        &\searrow&f_{3}&\rightarrow& Y_3& \\
                    \end{array}
                    \]
                </p>
                <p>
                    in which case \(I(X)\) is the set \(\{f_1, f_2\}\). Suppose furthermore that for each function \(f \in I(X)\), \(O\) is the output of \(f\) and the gradient \(\frac{\partial s}{\partial O}\) is already known. So in this case, we know 
                </p> -->
            </section>

        </main>

        <footer class="tutorial-footer">
            <p>© 2025 Michael Kosmider. All rights reserved.</p>
            <p><a href="#">Back to Top</a></p>
        </footer>
    </div>

    <script>
        // Optional: Smooth scrolling for TOC links
        document.querySelectorAll('.toc a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

</body>
</html>